{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import models.ViCCT_models  # Need to register the models!\n",
    "from models.Swin_ViCCT_models import Swin_ViCCT_large_22k\n",
    "from timm.models import create_model\n",
    "from datasets.dataset_utils import img_equal_unsplit\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ViCCT_base'  # Must be something like 'deit_small_distilled_patch16_224'.\n",
    "# model_path = 'D:\\\\OneDrive\\\\OneDrive - UvA\\\\ThesisData\\\\trained_models\\\\SWIN generic\\\\save_state_ep_1600.pth'\n",
    "model_path = 'D:\\\\OneDrive\\\\OneDrive - UvA\\\\ThesisData\\\\trained_models\\\\ViCCT base most public\\\\save_state_ep_1300.pth'\n",
    "label_factor = 3000  # The label factor used to train this specific model.\n",
    "dataset = 'Generic_ViCCT'  # Must be the exact name of the dataset\n",
    "save_results = False  # When true, save the images, GTs and predictions. A folder for this is created automatically.\n",
    "set_to_eval = 'test'  # val', 'test'. Which split to test the model on. 'train' does not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=model_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # Not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    ")\n",
    "model = model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = importlib.import_module(f'datasets.{dataset}.loading_data').loading_data\n",
    "cfg_data = importlib.import_module(f'datasets.{dataset}.settings').cfg_data\n",
    "\n",
    "train_loader, val_loader, test_loader, restore_transform = dataloader(model.crop_size)\n",
    "if set_to_eval == 'val':\n",
    "    my_dataloader = val_loader\n",
    "elif set_to_eval == 'test':\n",
    "    my_dataloader = test_loader\n",
    "else:\n",
    "    print(f'Error: invalid set --> {set_to_eval}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = None\n",
    "if save_results:\n",
    "    save_folder = 'DeiT' + '_' + dataset + '_' + set_to_eval + '_' + time.strftime(\"%m-%d_%H-%M\", time.localtime())\n",
    "    save_path = os.path.join('notebooks', save_folder)  # Manually change here is you want to save somewhere else\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_and_save_results(save_path, img, img_idx, gt, prediction, pred_cnt, gt_cnt):\n",
    "    img_save_path = os.path.join(save_path, f'IMG_{img_idx}_AE_{abs(pred_cnt - gt_cnt):.3f}.jpg')\n",
    "    \n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, 3, figsize=(13, 13))\n",
    "    axarr[0].imshow(img)\n",
    "    axarr[1].imshow(gt, cmap=cm.jet)\n",
    "    axarr[1].title.set_text(f'GT count: {gt_cnt:.3f}')\n",
    "    axarr[2].imshow(prediction, cmap=cm.jet)\n",
    "    axarr[2].title.set_text(f'predicted count: {pred_cnt:.3f}')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_save_path)\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, my_dataloader, show_predictions, restore_transform, label_factor, cfg_data):\n",
    "    loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    AEs = []  # Absolute Errors\n",
    "    SEs = []  # Squared Errors\n",
    "    GTs = []\n",
    "    preds = []\n",
    "    \n",
    "    crop_losses = []  # The loss of just the crops before recombining\n",
    "    whole_img_losses = []  # The loss of the image after the crops are combined\n",
    "    with torch.no_grad():\n",
    "        for idx, (img, img_patches, gt_patches) in enumerate(my_dataloader):\n",
    "\n",
    "            img_patches = img_patches.squeeze().cuda()\n",
    "            gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "            img = img.squeeze()  # Remove batch dimension\n",
    "            _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "            \n",
    "            img = restore_transform(img)\n",
    "\n",
    "            pred_stack = torch.zeros(img_patches.shape[0], 1, 224, 224)\n",
    "\n",
    "            \n",
    "            for idx2, img_crop in enumerate(img_patches):\n",
    "                pred_stack[idx2] = model.forward(img_crop.unsqueeze(0))\n",
    "#             pred_den = model(img_patches)  # Precicted density crops\n",
    "            pred_den = pred_stack.cpu()\n",
    "            \n",
    "            crop_loss = loss_fn(pred_den, gt_patches)\n",
    "            crop_loss = crop_loss.mean((-2, -1))\n",
    "            crop_losses.extend(crop_loss.tolist())\n",
    "\n",
    "            # Restore GT and Prediction\n",
    "            gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "            gt = gt.squeeze()  # Remove channel dim\n",
    "            den = den.squeeze()  # Remove channel dim\n",
    "            \n",
    "            whole_img_loss = loss_fn(den, gt)\n",
    "            whole_img_loss = whole_img_loss.mean((-2, -1))  # Only has 2 dims though\n",
    "            whole_img_losses.append(whole_img_loss.item())\n",
    "            \n",
    "\n",
    "            pred_cnt = den.sum() / label_factor\n",
    "            gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "            \n",
    "            AEs.append(torch.abs(pred_cnt - gt_cnt).item())\n",
    "            SEs.append(torch.square(pred_cnt - gt_cnt).item())\n",
    "            GTs.append(gt_cnt.item())\n",
    "            preds.append(pred_cnt.item())\n",
    "            relative_error = AEs[-1] / gt_cnt * 100\n",
    "            print(f'IMG {idx:<3} '\n",
    "                  f'Prediction: {pred_cnt:<9.3f} '\n",
    "                  f'GT: {gt_cnt:<9.3f} '\n",
    "                  f'Absolute Error: {AEs[-1]:<9.3f} '\n",
    "                  f'Relative Error: {relative_error:.1f}%')\n",
    "            \n",
    "            if save_path:\n",
    "                plot_and_save_results(save_path, img, idx, gt, den, pred_cnt, gt_cnt)\n",
    "            \n",
    "        MAE = np.mean(AEs)\n",
    "        MSE = np.sqrt(np.mean(SEs))\n",
    "        Mean_crop_loss = np.mean(crop_losses)\n",
    "        Mean_whole_img_loss = np.mean(whole_img_losses)\n",
    "\n",
    "    return MAE, MSE, Mean_crop_loss, Mean_whole_img_loss, GTs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MAE, MSE, Mean_crop_loss, Mean_whole_img_loss, GTs, preds = eval_model(model, my_dataloader, save_path, restore_transform, label_factor, cfg_data)\n",
    "print(f'MAE/MSE: {MAE}/{MSE}, Mean crop loss: {Mean_crop_loss:.3f}, Mean whole image loss: {Mean_whole_img_loss:.3f}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_nrs = np.arange(len(GTs))\n",
    "sorted_idxs = np.argsort(GTs)\n",
    "GTs = np.array(GTs)\n",
    "preds = np.array(preds)\n",
    "\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(img_nrs, GTs[sorted_idxs], label='Ground truths')\n",
    "plt.plot(img_nrs, preds[sorted_idxs], label='Predictions')\n",
    "plt.ylabel('Crowd count')\n",
    "plt.xlabel('Sorted image')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(f'DeiT_{dataset}_pred_vs_gt.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_error_idxs = np.flip(np.argsort(np.abs(GTs - preds)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in sorted_error_idxs[:10]:\n",
    "        img, img_patches, gt_patches = my_dataloader.dataset.__getitem__(idx)\n",
    "\n",
    "        img_patches = img_patches.cuda()\n",
    "        gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "        img = img.squeeze()  # Remove batch dimension\n",
    "        _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "\n",
    "        img = restore_transform(img)\n",
    "\n",
    "        pred_stack = torch.zeros(img_patches.shape[0], 1, 224, 224)\n",
    "\n",
    "\n",
    "        for idx2, img_crop in enumerate(img_patches):\n",
    "            pred_stack[idx2] = model.forward(img_crop.unsqueeze(0))\n",
    "#             pred_den = model(img_patches)  # Precicted density crops\n",
    "#             pred_den = pred_den.cpu()\n",
    "        pred_den = pred_stack.cpu()\n",
    "        \n",
    "        # Restore GT and Prediction\n",
    "        gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "        den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "        gt = gt.squeeze()  # Remove channel dim\n",
    "        den = den.squeeze()  # Remove channel dim\n",
    "\n",
    "\n",
    "        pred_cnt = den.sum() / label_factor\n",
    "        gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "\n",
    "        print(f'IMG {idx}, pred: {pred_cnt:.3f}, gt: {gt_cnt:.3f}. Error: {pred_cnt - gt_cnt:.3f}')\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(np.asarray(img))\n",
    "#         plt.savefig(f'DeiT_IMG_{idx + 1}_{dataset}.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(gt.numpy(), cmap=cm.jet)\n",
    "        plt.title(f'GT count: {gt_cnt:.3f}')\n",
    "#         plt.savefig(f'DeiT_IMG_{idx + 1}_{dataset}_prediction.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(den.numpy(), cmap=cm.jet)\n",
    "        plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "#         plt.savefig(f'DeiT_IMG_{idx + 1}_{dataset}_prediction.jpg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sorted_good_idxs = np.argsort(np.abs(GTs - preds))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in sorted_good_idxs[:20]:\n",
    "        img, img_patches, gt_patches = my_dataloader.dataset.__getitem__(idx)\n",
    "\n",
    "        img_patches = img_patches.cuda()\n",
    "        gt_patches = gt_patches.squeeze().unsqueeze(1)  # Remove batch dim, insert channel dim\n",
    "        img = img.squeeze()  # Remove batch dimension\n",
    "        _, img_h, img_w = img.shape  # Obtain image dimensions. Used to reconstruct GT and Prediction\n",
    "\n",
    "        img = restore_transform(img)\n",
    "\n",
    "        pred_stack = torch.zeros(img_patches.shape[0], 1, 224, 224)\n",
    "\n",
    "\n",
    "        for idx2, img_crop in enumerate(img_patches):\n",
    "            pred_stack[idx2] = model.forward(img_crop.unsqueeze(0))\n",
    "#             pred_den = model(img_patches)  # Precicted density crops\n",
    "#             pred_den = pred_den.cpu()\n",
    "        pred_den = pred_stack.cpu()\n",
    "        \n",
    "        # Restore GT and Prediction\n",
    "        gt = img_equal_unsplit(gt_patches, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "        den = img_equal_unsplit(pred_den, cfg_data.OVERLAP, cfg_data.IGNORE_BUFFER, img_h, img_w, 1)\n",
    "        gt = gt.squeeze()  # Remove channel dim\n",
    "        den = den.squeeze()  # Remove channel dim\n",
    "\n",
    "\n",
    "        pred_cnt = den.sum() / label_factor\n",
    "        gt_cnt = gt.sum() / cfg_data.LABEL_FACTOR\n",
    "\n",
    "        print(f'IMG {idx}, pred: {pred_cnt:.3f}, gt: {gt_cnt:.3f}. Error: {pred_cnt - gt_cnt:.3f}')\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(np.asarray(img))\n",
    "#         plt.savefig(f'DeiT_IMG_{idx + 1}_{dataset}.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(gt.numpy(), cmap=cm.jet)\n",
    "        plt.title(f'GT count: {gt_cnt:.3f}')\n",
    "#         plt.savefig(f'DeiT_IMG_{idx + 1}_{dataset}_prediction.jpg')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(15, 15))\n",
    "        plt.imshow(den.numpy(), cmap=cm.jet)\n",
    "        plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "#         plt.savefig(f'DeiT_IMG_{idx + 1}_{dataset}_prediction.jpg')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.cpu()\n",
    "# sd = model.state_dict()\n",
    "# save_d = {'state_dict': sd}\n",
    "# torch.save(save_d, '40_45_adapted.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
