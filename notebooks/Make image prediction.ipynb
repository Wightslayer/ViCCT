{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make image prediction\n",
    "\n",
    "Usually, we run a  model on an entire datasets to determine its performance. However, sometimes it is interesting to see what a model predicts on some random manually provided image. In this notebook, you can provide any image you like, specify which model to use, and observe the prediction. This notebook supports both CUDA and CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by going to the main directory of the project and import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wight\\PycharmProjects\\ViCCT\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import models.ViCCT_models\n",
    "import models.Swin_ViCCT_models\n",
    "from timm.models import create_model\n",
    "\n",
    "from datasets.dataset_utils import img_equal_split, img_equal_unsplit\n",
    "import torchvision.transforms as standard_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "Several parameters need to be defined to run this notebook. The cell below is the only cell that needs modification in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, which model will we use? \n",
    "# The generic ViCCT version 1 model is specified with 'ViCCT_base'. \n",
    "# The version 2 ViCCT model, which has Swin as its base, is specified with 'Swin_ViCCT_large_22k'.\n",
    "# model_name = 'ViCCT_base'\n",
    "model_name = 'Swin_ViCCT_large_22k'\n",
    "\n",
    "# The model is trained to perform crowd counting. We specify here where the weights of this trained model is located.\n",
    "# weights_path = 'models/trained_models/ViCCT_base_generic_1300_epochs.pth'\n",
    "weights_path = 'models/trained_models/Swin_ViCCT_large_22k_generic_1600_epochs.pth'\n",
    "\n",
    "# Now, for which image will the model make a prediction? We now specify where the image is located.\n",
    "image_path = '/PATH/TO/YOUR/IMAGE/FOLDER/image.png'\n",
    "\n",
    "# Some images are of extremely large resolution. When the heads in images occupy many (e.g. something like 100 x 100 \n",
    "# pixels each) pixels, the model is unable to make pretty predictions. One way to overcome this issue is to scale the image\n",
    "# by some factor. This factory is specified here. A factor of 1. means no scaling is performed.\n",
    "scale_factor = 1.\n",
    "\n",
    "# We might want to save the predictions. Set 'save_results' to true if you want to save the prediction. Three figures are saved\n",
    "# 1) The input image for the network. 2) The network's prediction. 3) The predictions overlayed with the input.\n",
    "save_results = True\n",
    "\n",
    "# Lastly, do we use cuda? If you have cuda, it's advised to use it.\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some variables used later. No need to modify these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for hardcore users. No need to modify these.\n",
    "mean_std = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Mean and std.dev. of ImageNet\n",
    "overlap = 32  # We ensure crops have at least this many pixels of overlap.\n",
    "ignore_buffer = 16  # When reconsturting the whole density map, ignore this many pixels on crop prediction borders.\n",
    "\n",
    "train_img_transform = standard_transforms.Compose([\n",
    "    standard_transforms.ToTensor(),\n",
    "    standard_transforms.Normalize(*mean_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to load the model and input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, weights_path, use_cuda):\n",
    "    \"\"\" Creates the model and initialised it with the weights specified. \"\"\"\n",
    "    \n",
    "    model = create_model(  # From the timm library. This function created the model specific architecture.\n",
    "    model_name,\n",
    "    init_path=weights_path,\n",
    "    pretrained_cc=True,\n",
    "    drop_rate=None if 'Swin' in model_name else 0.,  # Dropout\n",
    "\n",
    "    # Bamboozled by Facebook. This isn't drop_path_rate, but rather 'drop_connect'.\n",
    "    # I'm not yet sure what it is for the Swin version\n",
    "    drop_path_rate=None if 'Swin' in model_name else 0.,\n",
    "    drop_block_rate=None,  # Drops our entire Transformer blocks I think? Not used for ViCCT.\n",
    "    )\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()  # Place model on GPU\n",
    "    \n",
    "    model = model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_image(image_path, scale_factor):\n",
    "    \"\"\" Loads the image from disk. Scaled the image if specified. \"\"\"\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    if scale_factor != 1.:\n",
    "        w, h = img.size\n",
    "        new_w, new_h = round(w * scale_factor), round(h * scale_factor)\n",
    "        img = img.resize((new_w, new_h))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model and image. Next, make a prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the model...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'v'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-ecd52950a790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Making the model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Get the image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-70014b739667>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(model_name, weights_path, use_cuda)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# I'm not yet sure what it is for the Swin version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdrop_path_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;34m'Swin'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mdrop_block_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Drops our entire Transformer blocks I think? Not used for ViCCT.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ThesisMain\\lib\\site-packages\\timm\\models\\factory.py\u001b[0m in \u001b[0;36mcreate_model\u001b[1;34m(model_name, pretrained, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mset_layer_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscriptable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscriptable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexportable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexportable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_jit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_jit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\ViCCT\\models\\Swin_ViCCT_models.py\u001b[0m in \u001b[0;36mSwin_ViCCT_large_22k\u001b[1;34m(init_path, pretrained_cc, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minit_path\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpretrained_cc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mfull_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mfull_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrop_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\ViCCT\\models\\Swin_ViCCT_models.py\u001b[0m in \u001b[0;36mload_pretrained\u001b[1;34m(model, init_path)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;34m\"\"\" Loads a pretrained crowd counting network. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m     \u001b[0mresume_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ThesisMain\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    593\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ThesisMain\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    762\u001b[0m             \"functionality.\")\n\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m     \u001b[0mmagic_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: invalid load key, 'v'."
     ]
    }
   ],
   "source": [
    "# Get the model\n",
    "print('Making the model...')\n",
    "model = get_model(model_name, weights_path, use_cuda)\n",
    "\n",
    "# Get the image\n",
    "print('Loading the image...')\n",
    "input_image = get_image(image_path, scale_factor)\n",
    "img = input_image.copy()  # Make a copy so we don't alter the original\n",
    "img_w, img_h = img.size\n",
    "\n",
    "# Before we make the prediction, we normalise the image and split it up into crops\n",
    "print('Preparing image...')\n",
    "img = train_img_transform(img)\n",
    "img_stack = img_equal_split(img, 224, overlap)  # Split the image ensuring a minimum of 'overlap' of overlap between crops.\n",
    "\n",
    "if use_cuda:\n",
    "    img_stack = img_stack.cuda()  # Place image stack on GPU        \n",
    "    \n",
    "# This is the placeholder where we store the model predictions.\n",
    "pred_stack = torch.zeros(img_stack.shape[0], 1, 224, 224)\n",
    "\n",
    "print('Making prediction now...')\n",
    "if not use_cuda and img_stack.shape[0] > 100:  # If on CPU and more than 100 image crops.\n",
    "    print('\\033[93m'\n",
    "          'WARNING: you are making a prediction on the CPU but provided a large image. This might take a'\n",
    "          ' (very) long time! You might want to consider downsizing the image with \"scale_factor\".'\n",
    "          '\\033[0m')\n",
    "\n",
    "with torch.no_grad():  # Dont make gradients\n",
    "#     for idx, img_crop in enumerate(tqdm(img_stack)):  # For each image crop\n",
    "    for idx, img_crop in enumerate(img_stack):  # For each image crop\n",
    "        print(f'Processing part {idx} of {img_stack.shape[0]}')\n",
    "        pred_stack[idx] = model.forward(img_crop.unsqueeze(0)).cpu()  # Make prediction.\n",
    "print('Done!')\n",
    "\n",
    "\n",
    "# Unsplit the perdiction crops to get the entire density map of the image.\n",
    "den = img_equal_unsplit(pred_stack, overlap, ignore_buffer, img_h, img_w, 1)\n",
    "den = den.squeeze()  # Remove the channel dimension\n",
    "\n",
    "# Compute the perdicted count, which is the sum of the entire density map. Note that the model is trained with density maps\n",
    "# scaled by a factor of 3000 (See sec 5.2 of my thesis for why: https://scripties.uba.uva.nl/search?id=723178). In short,\n",
    "# This works :)\n",
    "pred_cnt = den.sum() / 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_func(image_path, input_image, prediction, predicted_count, overlayed):\n",
    "    \"\"\" Saves the results in the 'try_image_result' directory. Makes the directory if it doesn't exist. \"\"\"\n",
    "    \n",
    "    # We save results here\n",
    "    save_dir = 'try_image_result'\n",
    "    \n",
    "    # Make dir if not exists\n",
    "    if not os.path.exists(save_dir):  \n",
    "        os.mkdir(save_dir)\n",
    "        \n",
    "    # Extract just the filename from image path\n",
    "    full_name = os.path.basename(image_path)\n",
    "    file_name = os.path.splitext(full_name)[0]\n",
    "    \n",
    "    # The paths where to save the results\n",
    "    input_save_name = file_name + '_input' + '.jpg'\n",
    "    img_save_path = os.path.join(save_dir, input_save_name)\n",
    "    predicion_save_name = file_name + '_prediction' + '.jpg'\n",
    "    pred_save_path = os.path.join(save_dir, predicion_save_name)\n",
    "    overlayed_save_name = file_name + '_overlayed' + '.jpg'\n",
    "    overlayed_save_path = os.path.join(save_dir, overlayed_save_name)\n",
    "\n",
    "    \n",
    "    # Save results\n",
    "    fig = plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(input_image, cmap=cm.jet)\n",
    "    plt.title(f'Input image for the network')\n",
    "    plt.savefig(img_save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(prediction, cmap=cm.jet)\n",
    "    plt.title(f'Predicted count: {predicted_count:.3f}')\n",
    "    plt.savefig(pred_save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "    plt.imshow(overlayed, cmap=cm.jet)\n",
    "    plt.title(f'Predicted count: {predicted_count:.3f}')\n",
    "    plt.savefig(overlayed_save_path, dpi=200, bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show input image\n",
    "plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "plt.imshow(input_image, cmap=cm.jet)\n",
    "plt.title(f'Input image for the network')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model prediction\n",
    "plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "plt.imshow(den, cmap=cm.jet)\n",
    "plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTAL: Overlay the predicted density with the input image\n",
    "\n",
    "Overlaps the predicted density map with the input image. To achieve this, we decrease the values in the 'red' and 'green' channels for pixels where the predicted density is above a certain threshold, and replace the values in the 'blue' channel with the normalised values of the predicted density map where those values exceed a given threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_heat = np.array(input_image)\n",
    "den_heat = den.clone().numpy()\n",
    "\n",
    "den_heat = den_heat / 3000  # Scale values to original domain\n",
    "den_heat[den_heat < 0] = 0  # Remove negative values\n",
    "den_heat = den_heat / den_heat.max() # Normalise between 0 and 1\n",
    "\n",
    "den_heat **= 0.5  # Reduce large values, increase small values\n",
    "den_heat *= 255  # Values from 0 to 255 now\n",
    "den_heat[den_heat < 50] = 0  # Threshold of 50\n",
    "\n",
    "img_heat[:, :, 0][den_heat > 0] = img_heat[:, :, 0][den_heat > 0] / 2\n",
    "img_heat[:, :, 1][den_heat > 0] = img_heat[:, :, 1][den_heat > 0] / 2\n",
    "img_heat[:, :, 2][den_heat > 0] = den_heat[den_heat > 0]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(1920/200, 1080/200), dpi=200)\n",
    "plt.imshow(img_heat)\n",
    "plt.title(f'Predicted count: {pred_cnt:.3f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results if specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results if specified\n",
    "if save_results:\n",
    "    save_result_func(image_path, input_image, den, pred_cnt, img_heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ThesisMain",
   "language": "python",
   "name": "thesismain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
